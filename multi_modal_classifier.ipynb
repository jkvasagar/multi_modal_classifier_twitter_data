{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25203,"status":"ok","timestamp":1681808523871,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"UKubNlU7S_1N","outputId":"3db1145b-b495-4360-c0a9-59545ded0d26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"executionInfo":{"elapsed":2671,"status":"ok","timestamp":1681810949573,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"cxIH9_XMsxMk","outputId":"1a3aa2f8-38b1-4aa6-cfda-f914c13e03eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["             tweet_id              image_id        text_info  text_info_conf  \\\n","0  918000000000000000  917791044158185473_0      informative          1.0000   \n","1  918000000000000000  917793137925459968_0      informative          1.0000   \n","2  918000000000000000  917793137925459968_1      informative          1.0000   \n","3  918000000000000000  917793137925459968_2      informative          1.0000   \n","4  918000000000000000  917793736918216706_0  not_informative          0.6535   \n","\n","        image_info  image_info_conf                         text_human  \\\n","0      informative           0.6766         other_relevant_information   \n","1      informative           1.0000  infrastructure_and_utility_damage   \n","2      informative           0.6538  infrastructure_and_utility_damage   \n","3      informative           1.0000  infrastructure_and_utility_damage   \n","4  not_informative           1.0000                   not_humanitarian   \n","\n","   text_human_conf                        image_human  image_human_conf  \\\n","0           1.0000         other_relevant_information            0.6766   \n","1           1.0000  infrastructure_and_utility_damage            1.0000   \n","2           1.0000  infrastructure_and_utility_damage            0.6538   \n","3           1.0000  infrastructure_and_utility_damage            1.0000   \n","4           0.6535                   not_humanitarian            1.0000   \n","\n","    image_damage  image_damage_conf  \\\n","0            NaN                NaN   \n","1  severe_damage             1.0000   \n","2  severe_damage             1.0000   \n","3  severe_damage             0.6434   \n","4            NaN                NaN   \n","\n","                                          tweet_text  \\\n","0  RT @Gizmodo: Wildfires raging through Northern...   \n","1  RT @KAKEnews: California wildfires destroy mor...   \n","2  RT @KAKEnews: California wildfires destroy mor...   \n","3  RT @KAKEnews: California wildfires destroy mor...   \n","4  California wildfire. Ã¡Â½Â¡4 https://t.co/a8oD...   \n","\n","                                        image_url  \\\n","0  http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg   \n","1  http://pbs.twimg.com/media/DLtgmEPXUAEo1LV.jpg   \n","2  http://pbs.twimg.com/media/DLtgmEPXkAAvOdi.jpg   \n","3  http://pbs.twimg.com/media/DLtgmF9X0AASfbh.jpg   \n","4  http://pbs.twimg.com/media/DLyoiI0X0AAsw1h.jpg   \n","\n","                                          image_path  \n","0  data_image/california_wildfires/10_10_2017/917...  \n","1  data_image/california_wildfires/10_10_2017/917...  \n","2  data_image/california_wildfires/10_10_2017/917...  \n","3  data_image/california_wildfires/10_10_2017/917...  \n","4  data_image/california_wildfires/10_10_2017/917...  "],"text/html":["\n","  <div id=\"df-69dccdf6-728d-4612-a0c4-a0656cfa1077\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>image_id</th>\n","      <th>text_info</th>\n","      <th>text_info_conf</th>\n","      <th>image_info</th>\n","      <th>image_info_conf</th>\n","      <th>text_human</th>\n","      <th>text_human_conf</th>\n","      <th>image_human</th>\n","      <th>image_human_conf</th>\n","      <th>image_damage</th>\n","      <th>image_damage_conf</th>\n","      <th>tweet_text</th>\n","      <th>image_url</th>\n","      <th>image_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>918000000000000000</td>\n","      <td>917791044158185473_0</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>informative</td>\n","      <td>0.6766</td>\n","      <td>other_relevant_information</td>\n","      <td>1.0000</td>\n","      <td>other_relevant_information</td>\n","      <td>0.6766</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>RT @Gizmodo: Wildfires raging through Northern...</td>\n","      <td>http://pbs.twimg.com/media/DLyi_WYVYAApwNg.jpg</td>\n","      <td>data_image/california_wildfires/10_10_2017/917...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>918000000000000000</td>\n","      <td>917793137925459968_0</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>1.0000</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>1.0000</td>\n","      <td>severe_damage</td>\n","      <td>1.0000</td>\n","      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n","      <td>http://pbs.twimg.com/media/DLtgmEPXUAEo1LV.jpg</td>\n","      <td>data_image/california_wildfires/10_10_2017/917...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>918000000000000000</td>\n","      <td>917793137925459968_1</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>informative</td>\n","      <td>0.6538</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>1.0000</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>0.6538</td>\n","      <td>severe_damage</td>\n","      <td>1.0000</td>\n","      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n","      <td>http://pbs.twimg.com/media/DLtgmEPXkAAvOdi.jpg</td>\n","      <td>data_image/california_wildfires/10_10_2017/917...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>918000000000000000</td>\n","      <td>917793137925459968_2</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>informative</td>\n","      <td>1.0000</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>1.0000</td>\n","      <td>infrastructure_and_utility_damage</td>\n","      <td>1.0000</td>\n","      <td>severe_damage</td>\n","      <td>0.6434</td>\n","      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n","      <td>http://pbs.twimg.com/media/DLtgmF9X0AASfbh.jpg</td>\n","      <td>data_image/california_wildfires/10_10_2017/917...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>918000000000000000</td>\n","      <td>917793736918216706_0</td>\n","      <td>not_informative</td>\n","      <td>0.6535</td>\n","      <td>not_informative</td>\n","      <td>1.0000</td>\n","      <td>not_humanitarian</td>\n","      <td>0.6535</td>\n","      <td>not_humanitarian</td>\n","      <td>1.0000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>California wildfire. Ã¡Â½Â¡4 https://t.co/a8oD...</td>\n","      <td>http://pbs.twimg.com/media/DLyoiI0X0AAsw1h.jpg</td>\n","      <td>data_image/california_wildfires/10_10_2017/917...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69dccdf6-728d-4612-a0c4-a0656cfa1077')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-69dccdf6-728d-4612-a0c4-a0656cfa1077 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-69dccdf6-728d-4612-a0c4-a0656cfa1077');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":70}],"source":["import pandas as pd\n","import os.path as path\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torchvision import datasets, transforms\n","from torch.utils.data import random_split, DataLoader\n","\n","\n","data_set = pd.read_excel(\n","    '/content/drive/MyDrive/CrisisMMD_v2.0/Tag_matched.xlsx'\n","    )\n","path_for_image = '/content/drive/MyDrive/CrisisMMD_v2.0'\n","data_set.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQXTk9y8sxMm"},"outputs":[],"source":["data_set['image_human'].value_counts()\n","data_set = data_set[data_set['image_human'] != 'not_humanitarian']\n","data_set = data_set.dropna(subset=['tweet_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X8SO9QzwsxMm"},"outputs":[],"source":["labels_dict = {'affected_individuals': 0,'infrastructure_and_utility_damage':1, 'injured_or_dead_people': 2,\n","               'missing_or_found_people':3, 'other_relevant_information': 4,'rescue_volunteering_or_donation_effort':5,\n","                 'vehicle_damage': 6}\n","\n","def convert_gender_label(gender_str):\n","    return labels_dict.get(gender_str, -1)  # return -1 for unknown labels\n","label_tuple = tuple(data_set.image_human.values)\n","numerical_label = [convert_gender_label(i) for i in label_tuple]\n","numerical_label"]},{"cell_type":"markdown","metadata":{"id":"XIeRlOyVc0Rm"},"source":["# **Image model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qXlFawTsxMm"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","from torchvision import transforms\n","\n","IMAGE_WIDTH = 224\n","IMAGE_HEIGHT = 224\n","# If more preprocessing of image is requried,\n","# need to write a seprate function\n","\n","# define the transformations to be applied to the dataset\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, data_set, image_dir = path_for_image, transform = None):\n","        self.image_dir = image_dir\n","        self.data = data_set.copy()\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        row = self.data.iloc[index]\n","        image_path = path.join(path_for_image, row['image_path'])\n","        label = row['image_human']\n","\n","        # Load Image\n","        image = Image.open(image_path)\n","        image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT))\n","        image = image.convert('RGB')\n","\n","        label_tensor = torch.tensor(label)\n","        image = self.transform(image)\n","        return (image, label_tensor)\n","\n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjPXjtxVsxMm"},"outputs":[],"source":["data_set['image_human'] = numerical_label\n","full_data = ImageDataset(data_set, transform = transform)\n","train_len = int(0.7*len(data_set))\n","test_len = len(data_set) - int (0.8 *train_len)\n","val_len = len(data_set) - train_len - test_len\n","train_data, test_data, val_data = random_split(full_data, [train_len, test_len, val_len])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3d5SCiOsxMn"},"outputs":[],"source":["BATCH_SIZE = 32\n","LEARNING_RATE = 0.0001\n","NUM_EPOCH = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681813255335,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"jNfG2l64ezvf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b9250e7-08e0-4856-e556-c9fe56c88d16"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAjobbuvsxMn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681814807725,"user_tz":-330,"elapsed":1552393,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"7f52df8c-889d-4964-b847-b8690520801a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20 - Train Loss: 1.5228, Train Acc: 0.7552, Test Loss: 1.4176, Test Acc: 0.8584\n","Epoch 2/20 - Train Loss: 1.4070, Train Acc: 0.8688, Test Loss: 1.4297, Test Acc: 0.8418\n","Epoch 3/20 - Train Loss: 1.3921, Train Acc: 0.8826, Test Loss: 1.4131, Test Acc: 0.8654\n","Epoch 4/20 - Train Loss: 1.3769, Train Acc: 0.8973, Test Loss: 1.4176, Test Acc: 0.8523\n","Epoch 5/20 - Train Loss: 1.3768, Train Acc: 0.8980, Test Loss: 1.4487, Test Acc: 0.8252\n","Epoch 6/20 - Train Loss: 1.3789, Train Acc: 0.8950, Test Loss: 1.4162, Test Acc: 0.8593\n","Epoch 7/20 - Train Loss: 1.3612, Train Acc: 0.9130, Test Loss: 1.4007, Test Acc: 0.8759\n","Epoch 8/20 - Train Loss: 1.3777, Train Acc: 0.8961, Test Loss: 1.4240, Test Acc: 0.8488\n","Epoch 9/20 - Train Loss: 1.3622, Train Acc: 0.9119, Test Loss: 1.4092, Test Acc: 0.8636\n","Epoch 10/20 - Train Loss: 1.3564, Train Acc: 0.9186, Test Loss: 1.4168, Test Acc: 0.8575\n","Epoch 11/20 - Train Loss: 1.3646, Train Acc: 0.9096, Test Loss: 1.4502, Test Acc: 0.8252\n","Epoch 12/20 - Train Loss: 1.3586, Train Acc: 0.9149, Test Loss: 1.4149, Test Acc: 0.8593\n","Epoch 13/20 - Train Loss: 1.3553, Train Acc: 0.9186, Test Loss: 1.4270, Test Acc: 0.8444\n","Epoch 14/20 - Train Loss: 1.3502, Train Acc: 0.9254, Test Loss: 1.4251, Test Acc: 0.8479\n","Epoch 15/20 - Train Loss: 1.3487, Train Acc: 0.9246, Test Loss: 1.4099, Test Acc: 0.8619\n","Epoch 16/20 - Train Loss: 1.3462, Train Acc: 0.9269, Test Loss: 1.4170, Test Acc: 0.8558\n","Epoch 17/20 - Train Loss: 1.3424, Train Acc: 0.9314, Test Loss: 1.4222, Test Acc: 0.8505\n","Epoch 18/20 - Train Loss: 1.3462, Train Acc: 0.9280, Test Loss: 1.4087, Test Acc: 0.8645\n","Epoch 19/20 - Train Loss: 1.3368, Train Acc: 0.9366, Test Loss: 1.4281, Test Acc: 0.8453\n","Epoch 20/20 - Train Loss: 1.3488, Train Acc: 0.9254, Test Loss: 1.4234, Test Acc: 0.8505\n","Linear(in_features=512, out_features=8, bias=True)\n"]}],"source":["\n","class ResNetWithFC(nn.Module):\n","    def __init__(self, num_classes=1000, fc_hidden_dim=512):\n","        super(ResNetWithFC, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.fc1 = nn.Linear(2048, fc_hidden_dim)\n","        self.fc2 = nn.Linear(fc_hidden_dim, num_classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.resnet.conv1(x)\n","        x = self.resnet.bn1(x)\n","        x = self.resnet.relu(x)\n","        x = self.resnet.maxpool(x)\n","\n","        x = self.resnet.layer1(x)\n","        x = self.resnet.layer2(x)\n","        x = self.resnet.layer3(x)\n","        x = self.resnet.layer4(x)\n","\n","        x = self.resnet.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","\n","        return x\n","\n","resnet = ResNetWithFC(8, 512)\n","\n","resnet.to(device)\n","\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n","                          shuffle=False, num_workers=2, pin_memory = True)\n","\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n","                          shuffle=True, num_workers=2, pin_memory = True)\n","\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(resnet.parameters(), lr=LEARNING_RATE)\n","\n","for epoch in range(NUM_EPOCH):\n","    # Train the model for one epoch\n","    resnet.train()\n","    train_loss = 0.0\n","    train_acc = 0.0\n","    for images, labels in train_loader:\n","        images = images.to(device)\n","        labels = labels.type(torch.LongTensor)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = resnet(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        train_acc += torch.sum(preds == labels.data)\n","    train_loss = train_loss / len(train_loader.dataset)\n","    train_acc = train_acc.double() / len(train_loader.dataset)\n","\n","    # Evaluate the model on the test set\n","    resnet.eval()\n","    test_loss = 0.0\n","    test_acc = 0.0\n","    with torch.no_grad():\n","      for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.type(torch.LongTensor)\n","        labels = labels.to(device)\n","        outputs = resnet(images)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        test_acc += torch.sum(preds == labels.data)\n","    test_loss = test_loss / len(test_loader.dataset)\n","    test_acc = test_acc.double() / len(test_loader.dataset)\n","\n","    # Print the training and test accuracy and loss for this epoch\n","    print('Epoch {}/{} - Train Loss: {:.4f}, Train Acc: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.4f}'\n","      .format(epoch + 1, NUM_EPOCH, train_loss, train_acc, test_loss, test_acc))\n","\n","print(resnet.fc2)"]},{"cell_type":"markdown","metadata":{"id":"UlOmhmlX6Spw"},"source":["#**Multi modal**"]},{"cell_type":"markdown","metadata":{"id":"GERn7wy_73Yj"},"source":["## Text processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1681808588101,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"c0Uexq_CN-1b","outputId":"e25493fa-7aef-4d36-d521-24e95efde0e8"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["from transformers import AdamW, BertConfig\n","import torch\n","import math"],"metadata":{"id":"8YVLJTR_fgB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197,"referenced_widgets":["6ed76066fb5e46c7873575705c0d808b","d8689fcd54d847a09572d4eee483f65c","33a4ddf74f944efe8440181ce8c1c5f9","2309afa5b54f49ce88d6465587682f41","dfb65487db484351a792ea42a4841333","de40a90c354c4ae3baadcc658f66a53f","d5ac23c3a7b440b78d6ba56335db49ea","27a57a121cb64507929ec8ddce43c0ca","c8a5d5b4e33049dea28bae8da0e21524","4dc0944a80f64c0cb1bb1de94ba378a5","48a93b8c93db47e0bddcc13d624a86bc","ec1aa2f5cf434670b01b09655a1daa91","44463ed143154950b0483cdba924474f","5171e68734294962b794f6c80e8b2c5d","ac2747db0e74444a80626827880b9bbb","0f2aaea0deeb4393868455597aecbf92","0c9c06cb3f6d4290a4e1b267f7164668","b2324eec2ca24ebd9910312e1018e637","4fb6fca5976944059b9623478412832a","0aed649def3c4c41a1bd7d7efdb4ebfa","a0dcb82236a543748e8ee2df377a7210","40a71fe2d2034d03bfd5fd8db5d53f3e","392b31eb303f46cca17ee167fc501de3","03ee258c044f4b7a89e0b1ccca7146ef","4fd3d061a7a340198965acdeeb6c4198","daeafccc36c8440e9c68660c4c73bc6f","a42514c58fa24c6fb087466ccd4c15d4","5598bebd290e4fa3aeb185390f85ac85","8415fc8c6baa4b44b7b36020a91aaf7a","6b9927eff44946efa7e9884fe50db076","fae7ab0851524bc2b2a334f3f1ecc90d","07b89151c5254d0ab1341d46dc690dfe","f495d6935a37481ebe043b8f791f057c"]},"executionInfo":{"elapsed":1670,"status":"ok","timestamp":1681808600943,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"LLG8p8w061JQ","outputId":"fc9b94f4-73dd-48c5-e8b3-29af40d6ceb6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed76066fb5e46c7873575705c0d808b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1aa2f5cf434670b01b09655a1daa91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"392b31eb303f46cca17ee167fc501de3"}},"metadata":{}}],"source":["import transformers\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification\n","model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=8,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184,"referenced_widgets":["f9aed0ccc4a2439998dcbd2507dc7004","b8bc9dc21c0d4dce88e76dec769470aa","c59f5c44353d45fc86112439d9d1a9ee","47ff9a8ab90745198e374e692d32e54a","f010d7e203a146c6983bc5b9af96e718","92a873b26df946a79a2e3e04fe049081","31f9b7eae4c84a80be14ef820354823b","4dadb235dffe47b4bf9cb8ba686b6b04","d993bab8350743c39d82e8653f0c8ae9","7e68fc30d87048e0a0a62bea8c9a1c1f","aabad5e544014d3ca690b013680cd7f8"]},"id":"Gs124jE_flkA","executionInfo":{"status":"ok","timestamp":1681808604971,"user_tz":-330,"elapsed":4034,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"ccb31e19-f52f-4b55-e7f9-972c3cca5aad"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9aed0ccc4a2439998dcbd2507dc7004"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["data = data_set.copy()\n","sentences = data.tweet_text.values\n","labels = data.image_human.values\n","sentences = ['NA' if x != x else x for x in sentences]\n","type(labels)"],"metadata":{"id":"vGFsXwnGf5qL","executionInfo":{"status":"ok","timestamp":1681811227855,"user_tz":-330,"elapsed":422,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a520c930-b4b5-4de4-8520-dd0b76fe55b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["label_tuple = tuple(labels)\n","labels_dict = {'affected_individuals': 0,'infrastructure_and_utility_damage':1, 'injured_or_dead_people': 2,\n","               'missing_or_found_people':3,'not_humanitarian':4, 'other_relevant_information': 5,'rescue_volunteering_or_donation_effort':6,\n","                 'vehicle_damage': 7}\n","\n","def convert_gender_label(gender_str):\n","    return labels_dict.get(gender_str, -1)  # return -1 for unknown labels\n","\n","\n","numerical_label = [convert_gender_label(i) for i in label_tuple]\n","print(numerical_label)  # Output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_mEx07Sf_K-","executionInfo":{"status":"ok","timestamp":1681810864342,"user_tz":-330,"elapsed":447,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"69b41830-e707-4e1b-97c9-72085c2252b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"]}]},{"cell_type":"code","source":["# Load the  tokenizer\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"NWniA20IgJjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3702,"status":"ok","timestamp":1681811282923,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"wwroVjwk8gsj","outputId":"c0b30881-631d-416c-e658-591f89f57c54"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["input_ids=[]\n","attention_masks=[]\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(sent, add_special_tokens=True,\n","                                         max_length=64, pad_to_max_length=True,\n","                                         return_attention_mask=True,\n","                                         return_tensors='pt')\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","input_ids=torch.cat(input_ids,dim=0)\n","attention_masks=torch.cat(attention_masks,dim=0)\n","labels = torch.tensor(numerical_label)"]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZKomgw1wuhq","executionInfo":{"status":"ok","timestamp":1681811297644,"user_tz":-330,"elapsed":1009,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"512ec9c9-7dbb-4e32-bdb9-cbe709e0e777"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4, 1, 1,  ..., 4, 4, 4])"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","\n","dataset = TensorDataset(input_ids,attention_masks,labels)\n","\n","train_set, val_set = random_split(dataset, [int(0.9*len(dataset)),(len(dataset)-int(0.9*len(dataset)))])\n","train_set, test_set = random_split(train_set, [int(0.9*len(train_set)),(len(train_set)-int(0.9*len(train_set)))])\n","\n","\n","# Print the sizes of each set\n","print(\"Training set size:\", len(train_set))\n","print(\"Validation set size:\", len(val_set))\n","print(\"Test set size:\", len(test_set))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkpZkxSDgQp3","executionInfo":{"status":"ok","timestamp":1681811303319,"user_tz":-330,"elapsed":481,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"491314ab-c130-47bb-bf40-8e24efa4b38d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set size: 3086\n","Validation set size: 382\n","Test set size: 343\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set),batch_size=batch_size)\n","val_dataloader = DataLoader(val_set, sampler=SequentialSampler(val_set),batch_size=batch_size)"],"metadata":{"id":"4MXRlAjZgTJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = AdamW(model_bert.parameters(),lr=5e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRjqB6LegVKh","executionInfo":{"status":"ok","timestamp":1681811313981,"user_tz":-330,"elapsed":10,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"ae417283-2c3e-4472-d500-529d86918374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from  transformers import get_linear_schedule_with_warmup\n","epochs=4\n","total_steps= len(train_dataloader)*epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)"],"metadata":{"id":"ayWcCL7ZgWeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def flat_accuracy(preds,labels):\n","    pred_flat = np.argmax(preds,axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat)/len(labels_flat)"],"metadata":{"id":"hpL6ahoAgeCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)"],"metadata":{"id":"5WVTUkP4gg4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_bert.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"du3SK75FhEJf","executionInfo":{"status":"ok","timestamp":1681811325608,"user_tz":-330,"elapsed":865,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"59eb252f-d987-4313-a309-4664dbbdc80c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",")"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["training_stats=[]\n","for epoch_i in range(0,epochs):\n","    print(\" \\n Epoch {:}/{:} \\n Training....\".format(epoch_i+1,epochs))\n","    total_train_loss=0\n","    model_bert.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch[0].cuda()\n","        b_input_mask = batch[1].cuda()\n","        b_labels = batch[2].cuda()\n","\n","        model_bert.zero_grad()\n","\n","        result = model_bert(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels,return_dict=True)\n","        loss=result.loss\n","        logits=result.logits\n","\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model_bert.parameters(),1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss/len(train_dataloader)\n","    print('Average Training Loss = {0:.2f}'.format(avg_train_loss))\n","    print('Running Evaluation....')\n","\n","\n","    model_bert.eval()\n","    tot_eval_acc,tot_eval_loss,no_of_eval_steps = 0,0,0\n","\n","    for batch in val_dataloader:\n","\n","        b_input_ids = batch[0].cuda()\n","        b_input_mask = batch[1].cuda()\n","        b_labels = batch[2].cuda()\n","\n","        with torch.no_grad():\n","            result = model_bert(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels,return_dict=True)\n","\n","        loss=result.loss\n","        logits=result.logits\n","\n","        tot_eval_loss += loss.item()\n","\n","        logits=logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        tot_eval_acc += flat_accuracy(logits,label_ids)\n","\n","    avg_val_acc = tot_eval_acc/len(val_dataloader)\n","    print(\"Accuracy : {0:.2f}\".format(avg_val_acc))\n","    avg_val_loss = tot_eval_loss / len(val_set)\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training loss': avg_train_loss,\n","            'Valid loss': avg_val_loss,\n","            'Valid_acc': avg_val_acc,\n","\n","\n","        }\n","    )\n","\n","    avg_val_loss = tot_eval_loss/len(val_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UzT7qyuQglsA","executionInfo":{"status":"ok","timestamp":1681811460174,"user_tz":-330,"elapsed":133872,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"c1dcc1b1-1f09-49b6-a762-cd5a6039f1af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" \n"," Epoch 1/4 \n"," Training....\n","Average Training Loss = 0.32\n","Running Evaluation....\n","Accuracy : 0.97\n"," \n"," Epoch 2/4 \n"," Training....\n","Average Training Loss = 0.08\n","Running Evaluation....\n","Accuracy : 0.96\n"," \n"," Epoch 3/4 \n"," Training....\n","Average Training Loss = 0.03\n","Running Evaluation....\n","Accuracy : 0.98\n"," \n"," Epoch 4/4 \n"," Training....\n","Average Training Loss = 0.01\n","Running Evaluation....\n","Accuracy : 0.97\n"]}]},{"cell_type":"code","source":["# Save the fine-tuned model\n","import os\n","#os.makedirs(\"path/to/fine_tuned_bert\")\n","model_bert.save_pretrained(\"path/to/fine_tuned_bert\")"],"metadata":{"id":"DYNishKWg_8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"53oItJuJ9niz"},"source":["## Image processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zz24fip-9rMW","outputId":"9a4f675e-47ba-42b0-fed7-7f0976fb0263","executionInfo":{"status":"ok","timestamp":1681809997234,"user_tz":-330,"elapsed":939413,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3811/3811 [15:39<00:00,  4.06it/s]\n"]}],"source":["from PIL import Image\n","import torchvision as tv\n","from tqdm import tqdm\n","transform_img = tv.transforms.Compose([\n","    tv.transforms.Resize((224,224)),\n","    tv.transforms.ToTensor(),\n","    tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","image_tensors = []\n","for image_sub_path in tqdm(data_set['image_path']):\n","  image_path = path.join(path_for_image, image_sub_path)\n","  image = Image.open(image_path)\n","  image = image.convert('RGB')\n","  image = transform_img(image)\n","  image_tensors.append(image)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1684,"status":"ok","timestamp":1681811545551,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"_VMWEAZNND9i","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a0b4337-17ec-42cd-d117-26398e2f16eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3811, 3811, 3811, 3811)"]},"metadata":{},"execution_count":104}],"source":["len(input_ids), len(attention_masks), len(image_tensors), len(labels)\n","#image_tensors = torch.stack(image_tensors)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5KLsmaIAdbV"},"outputs":[],"source":["# Creating tensor dataset\n","from torch.utils.data import TensorDataset, random_split\n","\n","full_dataset = TensorDataset(\n","    input_ids,\n","    attention_masks,\n","    image_tensors,\n","    torch.LongTensor(labels)\n","    )\n","train_len = int(0.7*len(full_dataset))\n","test_len = len(full_dataset) - int (0.8 *len(full_dataset))\n","val_len = len(full_dataset) - train_len - test_len\n","train_data, test_data, val_data = random_split(full_dataset, [train_len, test_len, val_len])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cC8BSkYeGRJc"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertModel\n","\n","class MultiModalClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(MultiModalClassifier, self).__init__()\n","\n","        # Add a linear layer to the ResNet50 backbone to reduce the feature dimensionality\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Linear(2048, 512)\n","\n","        # Load the BERT model and freeze its parameters\n","        self.bert = BertModel.from_pretrained('path/to/fine_tuned_bert')\n","\n","        # Combine the BERT and ResNet50 outputs\n","        self.fc1 = nn.Linear(512+768, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, num_classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_masks, image_tensors):\n","\n","        # ResNet50\n","        with torch.no_grad():\n","            resnet_output = self.resnet(image_tensors)\n","\n","        # BERT\n","        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_masks)[1]\n","\n","        # Concatenate the BERT and ResNet50 outputs\n","        combined_output = torch.cat((bert_output, resnet_output), dim=1)\n","\n","        # Fully connected layers\n","        x = F.relu(self.fc1(combined_output))\n","        x = F.relu(self.fc2(x))\n","        output = self.fc3(x)\n","        output = self.softmax(output)\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":196656,"status":"ok","timestamp":1681812254458,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"},"user_tz":-330},"id":"wjiz_PQsLfG7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff407af8-4320-490a-87cf-8605455d9b60"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at path/to/fine_tuned_bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/5_____Training..........\n","Average Training Loss = 1.87 \n","Running Evaluation...\n","Validation Accuracy: 96.06%\n","\n","Epoch 2/5_____Training..........\n","Average Training Loss = 1.62 \n","Running Evaluation...\n","Validation Accuracy: 95.80%\n","\n","Epoch 3/5_____Training..........\n","Average Training Loss = 1.37 \n","Running Evaluation...\n","Validation Accuracy: 95.54%\n","\n","Epoch 4/5_____Training..........\n","Average Training Loss = 1.25 \n","Running Evaluation...\n","Validation Accuracy: 96.33%\n","\n","Epoch 5/5_____Training..........\n","Average Training Loss = 1.21 \n","Running Evaluation...\n","Validation Accuracy: 97.64%\n"]}],"source":["from torch.utils.data import DataLoader\n","import torch.optim as optim\n","\n","train_data, test_data, val_data\n","batch_size = 32\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model\n","model = MultiModalClassifier(7)\n","model.cuda()\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","stats=[]\n","# Train the model for a specified number of epochs\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    print(\"\\nEpoch {}/{}_____Training..........\".format(epoch+1, num_epochs))\n","\n","    total_train_loss=0\n","    # Set the model to train mode\n","    model.train()\n","\n","    # Iterate over the batches in the training data\n","    for batch in train_dataloader:\n","\n","        # Extract the inputs and targets\n","        input_ids = batch[0].cuda()\n","        attention_masks = batch[1].cuda()\n","        image_data = batch[2].cuda()\n","        targets = batch[3].cuda()\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Compute the model's predictions\n","        outputs = model(input_ids, attention_masks, image_data)\n","\n","        # Compute the loss\n","        loss = criterion(outputs, targets)\n","\n","        total_train_loss += loss.item()\n","        # Compute the gradients\n","        loss.backward()\n","\n","        # Update the parameters\n","        optimizer.step()\n","\n","    avg_train_loss = total_train_loss/len(train_dataloader)\n","    print('Average Training Loss = {0:.2f} '.format(avg_train_loss))\n","    print('Running Evaluation...')\n","    # Set the model to evaluation mode\n","    model.eval()\n","\n","    # Compute the validation accuracy\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for batch in val_dataloader:\n","            input_ids = batch[0].cuda()\n","            attention_masks = batch[1].cuda()\n","            image_data = batch[2].cuda()\n","            targets = batch[3].cuda()\n","            outputs = model(input_ids, attention_masks, image_data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += targets.size(0)\n","            correct += (predicted == targets).sum().item()\n","        val_acc = 100 * correct / total\n","\n","    # Print the loss and validation accuracy for each epoch\n","    print('Validation Accuracy: {:.2f}%'.format(val_acc))\n","    stats.append(\n","        {\n","            'epoch': epoch + 1,\n","            'training_loss': avg_train_loss,\n","            'val_acc': val_acc,\n","        }\n","    )\n","\n"]},{"cell_type":"code","source":["print(stats)"],"metadata":{"id":"3C1m56zQbuVe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681812299936,"user_tz":-330,"elapsed":425,"user":{"displayName":"Saikat Kumar Bhaumik","userId":"04132463331247253401"}},"outputId":"9fe6d78b-3147-4965-b0cc-bad1f24ffb47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'epoch': 1, 'training_loss': 1.8719795942306519, 'val_acc': 96.06299212598425}, {'epoch': 2, 'training_loss': 1.6244346683933621, 'val_acc': 95.8005249343832}, {'epoch': 3, 'training_loss': 1.3697825954073952, 'val_acc': 95.53805774278216}, {'epoch': 4, 'training_loss': 1.248588936669486, 'val_acc': 96.3254593175853}, {'epoch': 5, 'training_loss': 1.2144745900517417, 'val_acc': 97.63779527559055}]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ihh0gnNE0xw1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"mmd","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6ed76066fb5e46c7873575705c0d808b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d8689fcd54d847a09572d4eee483f65c","IPY_MODEL_33a4ddf74f944efe8440181ce8c1c5f9","IPY_MODEL_2309afa5b54f49ce88d6465587682f41"],"layout":"IPY_MODEL_dfb65487db484351a792ea42a4841333"}},"d8689fcd54d847a09572d4eee483f65c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de40a90c354c4ae3baadcc658f66a53f","placeholder":"​","style":"IPY_MODEL_d5ac23c3a7b440b78d6ba56335db49ea","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"33a4ddf74f944efe8440181ce8c1c5f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27a57a121cb64507929ec8ddce43c0ca","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8a5d5b4e33049dea28bae8da0e21524","value":231508}},"2309afa5b54f49ce88d6465587682f41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dc0944a80f64c0cb1bb1de94ba378a5","placeholder":"​","style":"IPY_MODEL_48a93b8c93db47e0bddcc13d624a86bc","value":" 232k/232k [00:00&lt;00:00, 3.24MB/s]"}},"dfb65487db484351a792ea42a4841333":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de40a90c354c4ae3baadcc658f66a53f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ac23c3a7b440b78d6ba56335db49ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27a57a121cb64507929ec8ddce43c0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8a5d5b4e33049dea28bae8da0e21524":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dc0944a80f64c0cb1bb1de94ba378a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48a93b8c93db47e0bddcc13d624a86bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec1aa2f5cf434670b01b09655a1daa91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44463ed143154950b0483cdba924474f","IPY_MODEL_5171e68734294962b794f6c80e8b2c5d","IPY_MODEL_ac2747db0e74444a80626827880b9bbb"],"layout":"IPY_MODEL_0f2aaea0deeb4393868455597aecbf92"}},"44463ed143154950b0483cdba924474f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c9c06cb3f6d4290a4e1b267f7164668","placeholder":"​","style":"IPY_MODEL_b2324eec2ca24ebd9910312e1018e637","value":"Downloading (…)okenizer_config.json: 100%"}},"5171e68734294962b794f6c80e8b2c5d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb6fca5976944059b9623478412832a","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0aed649def3c4c41a1bd7d7efdb4ebfa","value":28}},"ac2747db0e74444a80626827880b9bbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0dcb82236a543748e8ee2df377a7210","placeholder":"​","style":"IPY_MODEL_40a71fe2d2034d03bfd5fd8db5d53f3e","value":" 28.0/28.0 [00:00&lt;00:00, 1.08kB/s]"}},"0f2aaea0deeb4393868455597aecbf92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c9c06cb3f6d4290a4e1b267f7164668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2324eec2ca24ebd9910312e1018e637":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fb6fca5976944059b9623478412832a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aed649def3c4c41a1bd7d7efdb4ebfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0dcb82236a543748e8ee2df377a7210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40a71fe2d2034d03bfd5fd8db5d53f3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"392b31eb303f46cca17ee167fc501de3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_03ee258c044f4b7a89e0b1ccca7146ef","IPY_MODEL_4fd3d061a7a340198965acdeeb6c4198","IPY_MODEL_daeafccc36c8440e9c68660c4c73bc6f"],"layout":"IPY_MODEL_a42514c58fa24c6fb087466ccd4c15d4"}},"03ee258c044f4b7a89e0b1ccca7146ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5598bebd290e4fa3aeb185390f85ac85","placeholder":"​","style":"IPY_MODEL_8415fc8c6baa4b44b7b36020a91aaf7a","value":"Downloading (…)lve/main/config.json: 100%"}},"4fd3d061a7a340198965acdeeb6c4198":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b9927eff44946efa7e9884fe50db076","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fae7ab0851524bc2b2a334f3f1ecc90d","value":570}},"daeafccc36c8440e9c68660c4c73bc6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07b89151c5254d0ab1341d46dc690dfe","placeholder":"​","style":"IPY_MODEL_f495d6935a37481ebe043b8f791f057c","value":" 570/570 [00:00&lt;00:00, 21.6kB/s]"}},"a42514c58fa24c6fb087466ccd4c15d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5598bebd290e4fa3aeb185390f85ac85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8415fc8c6baa4b44b7b36020a91aaf7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b9927eff44946efa7e9884fe50db076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fae7ab0851524bc2b2a334f3f1ecc90d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07b89151c5254d0ab1341d46dc690dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f495d6935a37481ebe043b8f791f057c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9aed0ccc4a2439998dcbd2507dc7004":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8bc9dc21c0d4dce88e76dec769470aa","IPY_MODEL_c59f5c44353d45fc86112439d9d1a9ee","IPY_MODEL_47ff9a8ab90745198e374e692d32e54a"],"layout":"IPY_MODEL_f010d7e203a146c6983bc5b9af96e718"}},"b8bc9dc21c0d4dce88e76dec769470aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92a873b26df946a79a2e3e04fe049081","placeholder":"​","style":"IPY_MODEL_31f9b7eae4c84a80be14ef820354823b","value":"Downloading pytorch_model.bin: 100%"}},"c59f5c44353d45fc86112439d9d1a9ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dadb235dffe47b4bf9cb8ba686b6b04","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d993bab8350743c39d82e8653f0c8ae9","value":440473133}},"47ff9a8ab90745198e374e692d32e54a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e68fc30d87048e0a0a62bea8c9a1c1f","placeholder":"​","style":"IPY_MODEL_aabad5e544014d3ca690b013680cd7f8","value":" 440M/440M [00:02&lt;00:00, 169MB/s]"}},"f010d7e203a146c6983bc5b9af96e718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92a873b26df946a79a2e3e04fe049081":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31f9b7eae4c84a80be14ef820354823b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4dadb235dffe47b4bf9cb8ba686b6b04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d993bab8350743c39d82e8653f0c8ae9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e68fc30d87048e0a0a62bea8c9a1c1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabad5e544014d3ca690b013680cd7f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}